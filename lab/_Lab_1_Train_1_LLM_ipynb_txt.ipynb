{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhnguyenvv/NLP-A/blob/main/_Lab_1_Train_1_LLM_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Je_LzLyxQED_"
      },
      "source": [
        "# Lab 1: Working with Text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Họ tên: Nguyễn Thị Lan Anh </br>\n",
        "> MSSV: 21120198"
      ],
      "metadata": {
        "id": "8qR6yjQJRXbb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NdXUhtlZQEED"
      },
      "source": [
        "# Bài tập 1:\n",
        "\n",
        "Tiến hành train một LLM dựa trên GPT2 theo hướng dẫn dưới đây\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFmUPu-3QQ4a",
        "outputId": "608f705d-5f7a-4d8c-f377-a22ae9dec0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oR5U_fzUQEEE"
      },
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBXnRFpBQEEF",
        "outputId": "f7ad6b54-f4b3-4256-b5c8-e753a1e5a858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.2.1+cu121\n",
            "tiktoken version: 0.6.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EmCNI7jQEFE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZiMH8FpQEFE"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-pCDSZwvQEEO"
      },
      "source": [
        "- Load raw text we want to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfIq24T4QEEO",
        "outputId": "7f4e3a1f-e6e7-41dc-ffc6-75f32d4ac158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT5HEm51QEFL"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSEQQVB7QEFL"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3K6QSsdQEFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd6e67d-32dc-4ab7-9686-fc57361c14d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nirS5OgUQEFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14d970f-99f3-4f30-d784-83796cbd1970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXei79hcQEFM"
      },
      "outputs": [],
      "source": [
        "block_size = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(block_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZN45FmLQEFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c46bc4-2c7e-45fc-aa50-6bf098dc1e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhsz6N9wQEFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c86c2e-a6c8-4a82-a488-52720c355366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vmlWW5fcQEFN"
      },
      "source": [
        "# Bài tập 2:\n",
        "\n",
        "Thực hiện và ghi kết quả chi tiết các bước encode và decode chuỗi sau: \"Akwirw ier\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bài tập 2:\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"Akwirw ier\""
      ],
      "metadata": {
        "id": "CEC61G0aRy8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Encode\n"
      ],
      "metadata": {
        "id": "9TJKsySdRFCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = tokenizer.encode(text)\n",
        "print(nums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yePch-JnRCkn",
        "outputId": "8eb36178-2611-46ab-d1b1-0b4768419048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Decode"
      ],
      "metadata": {
        "id": "8I9EO-BQRPDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(nums)"
      ],
      "metadata": {
        "id": "VK0yqf8TwRXt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11f490b3-a10f-46e2-bddb-b59baf6f2684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Akwirw ier'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Các bước decode"
      ],
      "metadata": {
        "id": "PhEQi1GbRVf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lần lượt decode từng token ID"
      ],
      "metadata": {
        "id": "Wj4mPDVBRgg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        "for num in nums:\n",
        "   tokens.append(tokenizer.decode([num]))\n",
        "   print(f\"{num} -> '{tokens[-1]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQIK7nV3N34L",
        "outputId": "094e7796-8875-465e-f76a-7b22001f8c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33901 -> 'Ak'\n",
            "86 -> 'w'\n",
            "343 -> 'ir'\n",
            "86 -> 'w'\n",
            "220 -> ' '\n",
            "959 -> 'ier'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ghép các tokens lại thành text"
      ],
      "metadata": {
        "id": "3agWLRQXSFxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\".join(t for t in tokens)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuHRhKwNRwWC",
        "outputId": "2e78c637-fdea-4080-a6af-abef584ef507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Các bước encode"
      ],
      "metadata": {
        "id": "KskGwOweS-iP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ta có hàm encode tách text thành các tokens là các từ phụ hoặc ký tự riêng lẻ:"
      ],
      "metadata": {
        "id": "pbZDrbyOQhiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"'{text}' ->\", end = \" \")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcwcWl_XPvZq",
        "outputId": "6667c3d5-0318-47d8-ccbb-ad163a1b9c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Akwirw ier' -> ['Ak', 'w', 'ir', 'w', ' ', 'ier']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- chuyển các tokens thành ID"
      ],
      "metadata": {
        "id": "Rhv50In5Plqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IDs = []\n",
        "for token in tokens:\n",
        "   IDs+=(tokenizer.encode(token))\n",
        "   print(f\"'{token}' -> {IDs[-1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB1-vDbHUDeJ",
        "outputId": "e7f33721-2242-4156-ef23-d8178009e952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Ak' -> 33901\n",
            "'w' -> 86\n",
            "'ir' -> 343\n",
            "'w' -> 86\n",
            "' ' -> 220\n",
            "'ier' -> 959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- thu được"
      ],
      "metadata": {
        "id": "vReuM5lwUhGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(IDs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWNGenbrUhYn",
        "outputId": "439f021c-3ebb-44c8-924a-5d5b136ddb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/usr/bin/python3",
        "-m",
        "ipykernel",
        "--HistoryManager.enabled=False",
        "--matplotlib=inline",
        "-c",
        "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3 (system-wide)",
      "env": {},
      "language": "python",
      "metadata": {
        "cocalc": {
          "description": "Python 3 programming language",
          "priority": 100,
          "url": "https://www.python.org/"
        }
      },
      "name": "python3",
      "resource_dir": "/ext/jupyter/kernels/python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c2TpWoo9QEEM",
        "DenPrc9cQEE1",
        "b6fOcpQIQEE_",
        "YHA4m8sjQEFH"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
