{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhnguyenvv/NLP-A/blob/main/project/ai_ali.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjLr_-jz7r9W",
        "outputId": "99a0f415-8393-45c4-861f-82e4bfc07a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 1s (168 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio -q\n",
        "!pip install openai -q\n",
        "!pip install gtts -q\n",
        "!pip install pydub -q\n",
        "!pip install config -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gTENxSh492It"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import config\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import whisper\n",
        "from openai import OpenAI\n",
        "import requests, gdown\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load whisper model"
      ],
      "metadata": {
        "id": "NNtL67nSBNSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aAlxhz_lFejg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Checking if NVIDIA GPU is available\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "W_model = whisper.load_model(\"base\", device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F81WeS25R7fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb5bfdc-7edb-4d8f-9d71-d5a032ddae36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jpM-ZP0h6OXDyuAfN1V6U7fvkdZuLLpU\n",
            "To: /content/Samples/SampleCall1.mp3\n",
            "100%|██████████| 7.69M/7.69M [00:00<00:00, 24.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vtdKzRvIwINdWEmnKG-U-VhFkrzTLDRL\n",
            "To: /content/Samples/SampleCall2.mp3\n",
            "100%|██████████| 1.85M/1.85M [00:00<00:00, 182MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v7SsfHr-slNIjVZ7tnliNJXYetVgwVya\n",
            "To: /content/Samples/SampleCall3.mp3\n",
            "100%|██████████| 2.86M/2.86M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ],
      "source": [
        "#File Project1_Data.json\n",
        "file_id = [\"1jpM-ZP0h6OXDyuAfN1V6U7fvkdZuLLpU\",\n",
        "           \"1vtdKzRvIwINdWEmnKG-U-VhFkrzTLDRL\",\n",
        "           \"1v7SsfHr-slNIjVZ7tnliNJXYetVgwVya\"\n",
        "]\n",
        "datafilename =[ \"./Samples/SampleCall1.mp3\",\n",
        "        \"./Samples/SampleCall2.mp3\",\n",
        "        \"./Samples/SampleCall3.mp3\"]\n",
        "for i in range(len(file_id)):\n",
        "   with open(datafilename[i], 'wb') as f:\n",
        "     gdown.download(f\"https://drive.google.com/uc?id={file_id[i]}\", datafilename[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lLMjBhPzBTF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set your OpenAI API key\n",
        "client = OpenAI(api_key = \"sk-oGx8ZhLZ0xMahXqAF3tGcsTQhfO7WGIiRC0UlwtNYKJU9pxu\",\n",
        "                base_url=\"https://api.chatanywhere.tech/v1\")\n",
        "mess = \"\"\n",
        "with open('prompt.txt', 'r') as file:\n",
        "    mess = file.read()\n",
        "messages_system= {\"role\": \"system\",\n",
        "                        \"content\": mess\n",
        "                       }\n",
        "\n",
        "# Transcribe audio using OpenAI's Whisper\n",
        "def transcribe_audio(audio):\n",
        "    segment_length = 600000\n",
        "    # Open the audio file\n",
        "    audio_file = AudioSegment.from_file(audio)\n",
        "    # Get the duration of the audio file in milliseconds\n",
        "    duration_ms = len(audio_file)\n",
        "    # Calculate the number of segments needed\n",
        "    num_segments = math.ceil(duration_ms / segment_length)\n",
        "    # Create an empty string to hold the concatenated text\n",
        "    all_text = \"\"\n",
        "    # Split the audio file into segments\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_length\n",
        "        end = min((i + 1) * segment_length, duration_ms)\n",
        "        segment = audio_file[start:end]\n",
        "        segment.export(f\"segment_{i}.mp3\", format=\"mp3\")\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        audio_file = open(f\"segment_{i}.mp3\", \"rb\")\n",
        "        transcript = W_model.transcribe(f\"segment_{i}.mp3\")\n",
        "        all_text += transcript['text']\n",
        "    return all_text\n",
        "import csv\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    mic=None,\n",
        "    text = \"Simulator IELTS Speaking test\",\n",
        "):\n",
        "    global messages_system\n",
        "    result = ''\n",
        "    messages = [messages_system]\n",
        "    if len(history) != 0:\n",
        "      messages.append({\"role\": \"user\", \"content\": text})\n",
        "      with open('chat_transcript.csv', 'r') as f:\n",
        "        history = csv.reader(f)\n",
        "\n",
        "      for val in history:\n",
        "          if val[0]:\n",
        "              messages.append({\"role\": val[0], \"content\": val[1]})\n",
        "\n",
        "      if mic is not None:\n",
        "          audio = mic\n",
        "      elif  len(message[\"files\"])!= 0:\n",
        "          audio = message[\"files\"][0]\n",
        "          #return message[\"files\"][0]\n",
        "      else:\n",
        "          return \"You must either provide a mic recording or a file\"\n",
        "\n",
        "      if audio:\n",
        "        transcript = transcribe_audio(audio)\n",
        "      else:\n",
        "        transcript = message[\"text\"]\n",
        "      result = \"Transcript: \" + transcript+ \"\\n\\n\"\n",
        "\n",
        "      messages.append({\"role\": \"user\", \"content\": transcript})\n",
        "    else:\n",
        "      messages.append({\"role\": \"user\", \"content\": text})\n",
        "\n",
        "    response = \"\"\n",
        "    response = client.chat.completions.create(\n",
        "          messages= messages,\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "      )\n",
        "    systems_message = response.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": systems_message})\n",
        "    chat_transcript =list[tuple[str, str]]\n",
        "    for message in messages:\n",
        "        if message['role'] != 'system':\n",
        "            chat_transcript.append([message['role'] , message['content']])\n",
        "    with open('chat_transcript.csv', 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerows(chat_transcript)\n",
        "    return systems_message +\"\\n\\n\"+ result\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Speak here...\"),\n",
        "         gr.Radio([\"Simulator IELTS Speaking test\",\n",
        "                                    \"Analyze of an existing IELTS Speaking record\" ,\n",
        "                                   ], label=\"\"),\n",
        "    ],\n",
        "    title=\"AI Auditor\",\n",
        "    description=\"AI Alliance for Audio Analytics Team. Our project's objective is to conduct a mock IELTS Speaking test, adhering closely to the IELTS standards.\",\n",
        "    textbox = gr.MultimodalTextbox(interactive=True, file_types=[\"audio\"], placeholder=\"upload file...\"),\n",
        "\n",
        "    multimodal=True,\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug = True)\n"
      ],
      "metadata": {
        "id": "-aMUTzWsOuDG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "862f3543-77ac-4157-e92a-7a90c5dd4e4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://5e6fa623034fb6cce4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5e6fa623034fb6cce4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5e6fa623034fb6cce4.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo by gradio Interface"
      ],
      "metadata": {
        "id": "Vpx0mTSfBJuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenAI API key\n",
        "client = OpenAI(api_key = \"sk-oGx8ZhLZ0xMahXqAF3tGcsTQhfO7WGIiRC0UlwtNYKJU9pxu\",\n",
        "                base_url=\"https://api.chatanywhere.tech/v1\")\n",
        "with open('prompt.txt', 'r') as file:\n",
        "    mess = file.read()\n",
        "messages_system= {\"role\": \"system\",\n",
        "                        \"content\": mess\n",
        "                       }\n",
        "# Transcribe audio using OpenAI's Whisper\n",
        "def transcribe_audio(audio):\n",
        "    segment_length = 600000\n",
        "    # Open the audio file\n",
        "    audio_file = AudioSegment.from_file(audio)\n",
        "    # Get the duration of the audio file in milliseconds\n",
        "    duration_ms = len(audio_file)\n",
        "    # Calculate the number of segments needed\n",
        "    num_segments = math.ceil(duration_ms / segment_length)\n",
        "    # Create an empty string to hold the concatenated text\n",
        "    all_text = ''\n",
        "    # Split the audio file into segments\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_length\n",
        "        end = min((i + 1) * segment_length, duration_ms)\n",
        "        segment = audio_file[start:end]\n",
        "        segment.export(f\"segment_{i}.mp3\", format=\"mp3\")\n",
        "\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        audio_file = open(f\"segment_{i}.mp3\", \"rb\")\n",
        "        transcript = W_model.transcribe(f\"segment_{i}.mp3\")\n",
        "        all_text += transcript['text']\n",
        "\n",
        "    return all_text\n",
        "\n",
        "# Analyze transcript using OpenAI's GPT-3\n",
        "def analyze_transcript(transcript):\n",
        "    global messages_system\n",
        "    messages=[]\n",
        "    messages.append(messages_system)\n",
        "    messages.append({\"role\": \"user\", \"content\": transcript})\n",
        "    response = client.chat.completions.create(\n",
        "          messages= messages,\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "      )\n",
        "\n",
        "    systems_message = response.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": systems_message})\n",
        "\n",
        "    chat_transcript = \"\"\n",
        "    for message in messages:\n",
        "        if message['role'] != 'system':\n",
        "            chat_transcript += message['role'] + \": \" + message['content'] + \"\\n\\n\"\n",
        "    result = \"Transcript: \" + transcript['text'] + \"\\n\\n\"\n",
        "    return systems_message +\"\\n\\n\"+ result\n",
        "# Define the transcription and analysis function\n",
        "def transcribe_and_analyze(mic=None, file=None):\n",
        "    if mic is not None:\n",
        "        audio = mic\n",
        "    elif file is not None:\n",
        "        audio = file\n",
        "    else:\n",
        "        return \"You must either provide a mic recording or a file\"\n",
        "    # Transcribe the audio to text\n",
        "    transcript = transcribe_audio(audio)\n",
        "    # Analyze sentiment and summarize\n",
        "    analysis = analyze_transcript(transcript)\n",
        "    return analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "eBJcmbmItWqw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXwSF6rDsAG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02e614b1-f548-41ee-fe98-5de71dce4580"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c1e636cf66b62a39bf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c1e636cf66b62a39bf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 528, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1908, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1485, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-20-9d2711b03599>\", line 66, in transcribe_and_analyze\n",
            "    analysis = analyze_transcript(transcript)\n",
            "  File \"<ipython-input-20-9d2711b03599>\", line 53, in analyze_transcript\n",
            "    result = \"Transcript: \" + transcript['text'] + \"\\n\\n\"\n",
            "TypeError: string indices must be integers\n"
          ]
        }
      ],
      "source": [
        "\n",
        "io = gr.Interface(fn=transcribe_and_analyze,\n",
        "                      inputs=[\n",
        "                          gr.Audio(sources=\"microphone\", type=\"numpy\", label=\"Speak here...\"),\n",
        "                          gr.Audio(sources=\"upload\", type = \"filepath\"),\n",
        "                      ],\n",
        "                      outputs=\"text\",\n",
        "                      title=\"AI Auditor for\",\n",
        "                      description='''AI Alliance for Audio Analytics Team. Our project's objective is to\n",
        "                      ''',\n",
        "                      examples=[[None, \"./Samples/SampleCall1.mp3\"],\n",
        "                                [None, \"./Samples/SampleCall2.mp3\"],\n",
        "                                [None, \"./Samples/SampleCall3.mp3\"]],\n",
        "                      )\n",
        "\n",
        "io.launch(share=True, debug = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}